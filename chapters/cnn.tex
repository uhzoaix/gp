\chapter{卷积神经网络}

\section{神经网络简介}

在介绍神经网络之前，我们先重新规范化下问题。给定N组训练数据，$\{x_i, y_i\}_{i=1}^N$，$x_i$是大小为$(n, w, h, c)$\footnote{n代表图像数量，w,h为图像的宽高，一般要求w,h相等，c则是图像的通道数，如RGB图像有3个通道，而灰度图像仅有一个通道}的四维张量，一般即代表输入的所有图像，而$y_i \in \mathbb{R}^M$则是图像的正确标签，$M$为类别的数量。现在需要解决的即是一个函数拟合的问题，即找到$F$，在给定的损失函数下，使如下总误差最小：

\[
L =\frac{1}{N}\sum_{i=1}^N loss(F(x_i), y_i)
\]

在一般的函数拟合问题中，$F$所属的函数空间会被限制，如限制到给定最高次数的多项式空间。而在深度学习领域中，我们则是将$F$限制在所谓“神经网络函数”空间\footnote{当然并没有如此的定义，神经网络的内涵如今还在扩充中，并不能严格的定义它}，其特征就是$F$是几种预先给定形式的函数的复合。比如，全连接层是具有如下形式的函数：

\[
	y = \sigma (x \cdot W + b)
\]

其中，x,y为输入输出的二维张量，形状分别为$(num,\  n)$与$(num,\  m)$，$W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^m $均是待定参数，$\sigma$是激活函数\footnote{神经网络中的非线性函数，常见的有Sigmoid函数,$S(t) = \frac{1}{1 + \exp(-t)}$ 与ReLU函数，$f(t) = \max(0, t)$}，在这里作用张量，即分别作用到各个分量上。

可以看出对全连接层而言，$W,b$完全确定了函数本身。因此，在优化损失函数$L$时，所需优化的变量即是所有网络层的参数，这便简化了问题，而且由于激活函数的导数易于计算，使得在计算机上实现快速求解成为可能。

而损失函数的选择一般是依赖于问题的，比如，对于图像分类问题而言，人们习惯选择交叉熵(Cross Entropy)来作为损失函数，即总误差L如下定义:
\[
	L = \frac1N \sum_{i=1}^N - \sum_{j=1}^{M} y_j \log (F(x_i)_j)
\]

在得到总误差的完整表达式后，我们就能将它作为目标函数，以数值优化的方法求得其最小值，更新其参数，这种优化求解，更新参数的过程在深度学习中即被称为训练。我们将在之后的章节更加详细的讨论关于优化的话题。

\section{CNN基本概念}

卷积神经网络，与一般的神经网络相比，多了卷积层的结构。下面将详细的介绍卷积神经网络中常见的概念，包括卷积层，池化层及为防止过拟合而进行的操作等。然后再说明对于目标函数的数值优化方法，并介绍其实现的数值原理，即误差向后传播算法(Backward Propagation of Errors)。

\subsection{建立模型}

一个简单的CNN数学模型的建立，即是各个层函数复合而成，这可以形象的描述为“将各个层按顺序叠起来”，为此我们要求每一层输入张量与其上一层输出张量的形状相同。如图(图片)即是LeNet的网络结构，它由2层卷积层(后接池化层)与2层全连层组成。在前面神经网络简介的章节里，我们已经介绍了全连层的概念，下面将介绍其他在CNN中常见的函数(层)。

\textbf{卷积层} \ 卷积层的概念与卷积运算密切相关。对于两个$\mathbb{L}^2$中的函数$f,g$而言，其卷积运算产生出新的函数按如下定义:
\[
	f * g \ (t) = \int_{\mathbb{R}} f(x) g(t-x) \  dx
\]

若f,g是二维张量，我们同样可以定义离散卷积：
\[
	f * g \ (x, y) = \sum_{i, j \in N(x,y), bdd(f)} f(i,j) \ g(x-i, y-j)
\]

这里$i,j \in N(x,y)$表示所有与$(x,y)$相邻且在f范围内\footnote{严格来讲为，$\{ \forall (i,j),|x-i| \le 1, |y-j| \le 1 \}$}的$(i,j)$。在图像处理中，高斯模糊即应用了离散卷积。一个灰度图像可以自然的看成一个矩阵f，然后定义一个方阵g\footnote{常见取的大小为3x3,5x5等，取决于具体应用，一般长宽为奇数}，称作卷积核，其值由二维高斯函数确定\footnote{去查，与上个footnote合并}，由于有限支集高斯函数具有低通滤波的特性，因此f和g做卷积后就能去除f的高频信息，如细节，噪声等。

与高斯模糊类似，卷积层函数也同样定义了自己的卷积核，并进行卷积操作得到新的张量。然而与前者相比，主要有如下几点不同：
\begin{enumerate}
	\item 卷积层可以有多个卷积核，每个卷积核对应输出张量的一个通道
	\item 该卷积核的值是未定参数，需要由误差函数优化得到
	\item 输入张量有多通道时，将各个通道做卷积操作后的值加起来作为结果
	\item 进行卷积操作后，需加入待定偏差参数b，然后被激活函数作用
\end{enumerate}

为了阐明清楚这些不同，我们写出其表达式。假设输入的一张图像$I$，大小为$(w_I, \ h_I, \ c_I)$，卷积核为$W^C = {(w^C_{ij})}_{k \times k}$，k为给定的卷积核大小，输出张量J，大小为$(w_J, \ h_J, \ c_J)$，则有如下表达式：
\[
	J(x,y,C) = \sigma(\sum_{1 \le c_1 \le C_I} I(\cdot, \cdot, c_1) * W^C \ (x,y) + b^C)
\]

其中，$C$的数量由外部给定，$W^C,b^C$为待定的参数，$\sigma$为激活函数。由此可以看出，卷积层的参数远少于全连层，这使得训练的时间大大减少，并且，卷积操作只涉及一小片区域，因此卷积层作用的结果更易体现局部特征（图，可视化的结果）。关于卷积层，还可以调整步长(strides)与填充(padding)等参数，我们放到附录中说明。

\textbf{池化层} \ 池化层(Pooling Layer)，常见的有极大池化层(Max Pooling)与平均池化层(mean Pooling)。为了说明池化层如何工作，考虑在一个输入的图像上一个可以滑动的小窗口，大小为$2 \times 2$。窗口每滑到一个位置，就拿窗口中的4个数作为输入，输出一个值。对于极大池化层，就是输出其最大值，对于平均池化层，就是输出4个值的平均值。设输入图像为I，输出图像为J，则极大池化层的表达式可以写为：
\[
	J(x,\ y) = \max_{i,j \in W(x,y)}I(i,j)
\]

其中，$i,j \in W(x,y)$即由窗口位置决定的所有$(i,j)$，注意输出图像的尺寸会由此缩小，缩小的程度取决于窗口的大小与滑动的步长等因素。

池化层的作用大致有两个方面：
\begin{enumerate}
	\item 对特征进行采样，从而起到降维的作用，使数据规模与参数减少
	\item 对特征进行整合，同时保持相对位置，减弱了抽取的特征对于空间位置的敏感性，从而在一定程度上，使特征具有空间不变性(如平移，旋转，放缩等)
\end{enumerate}

\textbf{防止过拟合的操作} \ 过拟合现象即在训练集上的误差很小，却在测试集上的误差变得较大。举例来说，现在要训练一个识别动物的CNN模型，但由于训练时过拟合，导致模型仅在训练集的图片上误差较小，而在新的同种类动物的图片上测试时，却无法识别。为了防止过拟合现象，常用的手段包括，数据增强(属于数据预处理的一部分)，误差加入正则项，加入Dropout层，以及调整学习速率(learning rate)等。这里对于Dropout与正则项进行说明。

\emph{Dropout} \ 

\subsection{优化与误差向后传播算法}