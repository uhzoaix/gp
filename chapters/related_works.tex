\chapter{相关工作}

上世纪90年代，LeCun在MNIST\footnote{Mixed National Institute of Standards and Technology，一个业界流行的手写数字数据库}上使用了LeNet，使得在其上的准确率提高到99\%。然而，由于受到当时计算能力的制约，CNN的技术并没有被进一步发展。直至2012年，Krizhevsky提出了Alexnet，并在当时计算机视觉领域的一项竞赛\footnote{ILSVRC2012，测试数据集为ImageNet}中取得了第一名，其测试Top-5误差\footnote{即给一张图片，如果输出的5个预测能猜对它的正确标签，那么就判定为正确}达到了15.4\%，而第二名则有26.2\%。从网络结构上看，Alexnet拥有5个卷积层，这使得它对图像特征的表达能力相对于LeNet大大增强，并且能处理更高分辨率的图像。此外，为了解决模型过拟合的问题，Alexnet对全连接层加入Dropout操作，这在以后被证明处理过拟合时十分有效。

随后，CNN进入了高速发展的时期。2013年，Zeller与Fergus提出了ZFnet，其本身是Alexnet的改进，其错误率降低到11.2\%，并且所需训练的样本由Alexnet的1500万降到130万，而它的网络结构并未太大变化。ZFnet的另一个重要贡献在于，提出用于可视化网络中的参数与特征的技术DeconvNet。2014年，Simonyan与Zisserman提出了VGG-net，并达到了7.3\%的误差。与之前的网络结构相比，VGG-net并没有引入新的结构与函数，而是大量增多卷积层(共19层卷积层)，使网络变“深”，其代价则是需要2到3个星期时间来训练网络。

而到了2015年，Google提出的GoogLeNet与微软提出的ResNet，大幅刷新了之前的记录，GoogLeNet的错误率达到6.7\%(在改进的版本Inception v3中，达到3.46\%)，同时ResNet达到了3.6\%,而一般人类的错误率仅在5\% \textasciitilde 10\%之间。GoogLeNet不同于以往的CNN，由卷积层，全连层等直接顺序叠加，对于前一层的输入，它并行的进行多种卷积与池化操作，并最后聚合起来，这使得网络变“宽”。另一方面，GoogLeNet舍弃了全连层，并以平均池化层(Mean Pooling Layer)代替，这使得需训练的参数大大减少。而ResNet则换了一种思路，它借鉴了残差学习中的思想，提出了残差块的结构，用于拟合输入输出之间的差。ResNet即是残差块的大量叠加，并最终达到152层。然而作者也指出，单纯的增加层数只会降低准确率，并更可能引起过拟合，他们尝试了1202层的网络，而其准确率确实不升反降。